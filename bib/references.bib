@INPROCEEDINGS{Abeni2006,
  keywords = {type:others, Authentication,Face recognition,Biometrics,Support vector machines,Support vector machine classification,Protection,Operating systems,Mobile handsets,Internet,Telecommunications},
  author={Abeni, Paolo and Baltatu, Madalina and D'Alessandro, Rosalia},
  booktitle={IEEE Globecom 2006},
  title={NIS03-4: Implementing Biometrics-Based Authentication for Mobile Devices},
  year={2006},
  ISSN={1930-529X},
  publisher={IEEE},
  conference_date={27 November 2006 - 01 December 2006},
  added_to_ieeexplore={16 April 2007},
  doi = {10.1109/GLOCOM.2006.276},
  url = {https://doi.org/10.1109/GLOCOM.2006.276},
  abstract = {This paper presents a face recognition system for mobile devices running the Symbian operating system. The recognition algorithm is based on a one-class classification approach implemented by means of support vector machines. It discusses important aspects related to the practical implementation of the algorithm on Symbian 60 Series mobile phones, and gives some preliminary results outlining the performances achieved by the recognition system with several different configurations.},
  note = {Published in: IEEE Globecom 2006},
  isbn = {1-4244-0357-X (CD); 1-4244-0356-1 (Print)},
  conference_dates = {27 November 2006 - 01 December 2006},
  date_added_to_ieeexplore = {16 April 2007},
}


@article{biggio2012security,
  title={Security evaluation of biometric authentication systems under real spoofing attacks},
  author={Biggio, B. and Akhtar, Z. and Fumera, G. and Marcialis, G.L. and Roli, F.},
  journal={IET Biometrics},
  volume={1},
  number={1},
  pages={11-24},
  year={2012},
  publisher={Institution of Engineering and Technology},
  doi={10.1049/iet-bmt.2011.0012},
  issn={2047-4938},
  url={https://doi.org/10.1049/iet-bmt.2011.0012},
  abstract={Multimodal biometric systems are commonly believed to be more robust to spoofing attacks than unimodal systems, as they combine information coming from different biometric traits. Recent work has shown that multimodal systems can be misled by an impostor even by spoofing only one biometric trait. This result was obtained under a ‘worst-case’ scenario, by assuming that the distribution of fake scores is identical to that of genuine scores (i.e. the attacker is assumed to be able to perfectly replicate a genuine biometric trait). This assumption also allows one to evaluate the robustness of score fusion rules against spoofing attacks, and to design robust fusion rules, without the need of actually fabricating spoofing attacks. However, whether and to what extent the ‘worst-case’ scenario is representative of real spoofing attacks is still an open issue. In this study, we address this issue by an experimental investigation carried out on several data sets including real spoofing attacks, related to a multimodal verification system based on face and fingerprint biometrics. On the one hand, our results confirm that multimodal systems are vulnerable to attacks against a single biometric trait. On the other hand, they show that the ‘worst-case’ scenario can be too pessimistic. This can lead to two conservative choices, if the ‘worst-case’ assumption is used for designing a robust multimodal system. Therefore developing methods for evaluating the robustness of multimodal systems against spoofing attacks, and for designing robust ones, remain a very relevant open issue.},
  keywords={type:2D spoofing experiment, face recognition, fingerprint identification, security of data, biometric authentication system, worst-case scenario, real spoofing attacks, fingerprint biometrics, multimodal verification system, security evaluation, score fusion rules, unimodal system, biometric traits, genuine scores, fake scores, face biometrics, robust fusion rules, multimodal biometric system}
}


@ARTICLE{6810829,
  author={Erdogmus, Nesli and Marcel, Sébastien},
  journal={IEEE Transactions on Information Forensics and Security},
  title={Spoofing Face Recognition With 3D Masks},
  year={2014},
  volume={9},
  number={7},
  pages={1084-1097},
  keywords={type:3D or 3D combine 2D spoofing experiment, Three-dimensional displays, Databases, Face recognition, Face, Solid modeling, Videos, Materials, Spoofing, presentation attack, mask attack}
  doi={10.1109/TIFS.2014.2322255},
  ISSN={1556-6021},
  month={July},
  publisher={IEEE},
  url={https://doi.org/10.1109/TIFS.2014.2322255},
  abstract={Spoofing is the act of masquerading as a valid user by falsifying data to gain an illegitimate access. Vulnerability of recognition systems to spoofing attacks (presentation attacks) is still an open security issue in biometrics domain and among all biometric traits, face is exposed to the most serious threat, since it is particularly easy to access and reproduce. In this paper, many different types of face spoofing attacks have been examined and various algorithms have been proposed to detect them. Mainly focusing on 2D attacks forged by displaying printed photos or replaying recorded videos on mobile devices, a significant portion of these studies ground their arguments on the flatness of the spoofing material in front of the sensor. However, with the advancements in 3D reconstruction and printing technologies, this assumption can no longer be maintained. In this paper, we aim to inspect the spoofing potential of subject-specific 3D facial masks for different recognition systems and address the detection problem of this more complex attack type. In order to assess the spoofing performance of 3D masks against 2D, 2.5D, and 3D face recognition and to analyze various texture-based countermeasures using both 2D and 2.5D data, a parallel study with comprehensive experiments is performed on two data sets: the Morpho database which is not publicly available and the newly distributed 3D mask attack database.}
}

@inproceedings{Chen2014,
  author = {Chen, Shaxun and Pande, Amit and Mohapatra, Prasant},
  title = {Sensor-assisted facial recognition: an enhanced biometric authentication system for smartphones},
  year = {2014},
  isbn = {9781450327930},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2594368.2594373},
  doi = {10.1145/2594368.2594373},
  booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
  pages = {109--122},
  numpages = {14},
  keywords = {type:2D anti-spoofing experiment, biometric, mobile devices, mobile sensors, user authentication},
  location = {Bretton Woods, New Hampshire, USA},
  series = {MobiSys '14},
  abstract = {Facial recognition is a popular biometric authentication technique, but it is rarely used in practice for device unlock or website/app login in smartphones, although most of them are equipped with a front-facing camera. Security issues (e.g. 2D media attack and virtual camera attack) and ease of use are two important factors that impede the prevalence of facial authentication in mobile devices. In this paper, we propose a new sensor-assisted facial authentication method to overcome these limitations. Our system uses motion and light sensors to defend against 2D media attacks and virtual camera attacks without the penalty of authentication speed. We conduct experiments to validate our method. Results show 95-97\% detection rate and 2-3\% false alarm rate over 450 trials in real-settings, indicating high security obtained by the scheme ten times faster than existing 3D facial authentications (3 seconds compared to 30 seconds).}
}


@inproceedings{Li2014,
  author = {Li, Yan and Xu, Ke and Yan, Qiang and Li, Yingjiu and Deng, Robert H.},
  title = {Understanding OSN-based facial disclosure against face authentication systems},
  year = {2014},
  isbn = {9781450328005},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2590296.2590315},
  doi = {10.1145/2590296.2590315},
  booktitle = {Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security},
  pages = {413--424},
  numpages = {12},
  keywords = {type:2D spoofing experiment,OSN-based facial disclosure, face authentication, online social networks},
  location = {Kyoto, Japan},
  series = {ASIA CCS '14},
  abstract = {Face authentication is one of promising biometrics-based user authentication mechanisms that have been widely available in this era of mobile computing. With built-in camera capability on smart phones, tablets, and laptops, face authentication provides an attractive alternative of legacy passwords for its memory-less authentication process. Although it has inherent vulnerability against spoofing attacks, it is generally considered sufficiently secure as an authentication factor for common access protection. However, this belief becomes questionable since image sharing has been popular in online social networks (OSNs). A huge number of personal images are shared every day and accessible to potential adversaries. This OSN-based facial disclosure (OSNFD) creates a significant threat against face authentication. In this paper, we make the first attempt to quantitatively measure the threat of OSNFD. We examine real-world face-authentication systems designed for both smartphones, tablets, and laptops. Interestingly, our results find that the percentage of vulnerable images that can used for spoofing attacks is moderate, but the percentage of vulnerable users that are subject to spoofing attacks is high. The difference between systems designed for smartphones/tablets and laptops is also significant. In our user study, the average percentage of vulnerable users is 64\% for laptop-based systems, and 93\% for smartphone/tablet-based systems. This evidence suggests that face authentication may not be suitable to use as an authentication factor, as its confidentiality has been significantly compromised due to OSNFD. In order to understand more detailed characteristics of OSNFD, we further develop a risk estimation tool based on logistic regression to extract key attributes affecting the success rate of spoofing attacks. The OSN users can use this tool to calculate risk scores for their shared images so as to increase their awareness of OSNFD.}
}


@article{7031384,
  author = {Wen, Di and Han, Hu and Jain, Anil K.},
  title = {Face Spoof Detection With Image Distortion Analysis},
  journal = {IEEE Transactions on Information Forensics and Security},
  volume = {10},
  number = {4},
  pages = {746--761},
  year = {2015},
  month = {April},
  date = {2015-02-04},
  doi = {10.1109/TIFS.2015.2400395},
  publisher = {IEEE},
  issn = {1556-6013},
  eissn = {1556-6021},
  url = {https://doi.org/10.1109/TIFS.2015.2400395},
  abstract = {Automatic face recognition is now widely used in applications ranging from deduplication of identity to authentication of mobile payment. This popularity of face recognition has raised concerns about face spoof attacks (also known as biometric sensor presentation attacks), where a photo or video of an authorized person's face could be used to gain access to facilities or services. While a number of face spoof detection techniques have been proposed, their generalization ability has not been adequately addressed. We propose an efficient and rather robust face spoof detection algorithm based on image distortion analysis (IDA). Four different features (specular reflection, blurriness, chromatic moment, and color diversity) are extracted to form the IDA feature vector. An ensemble classifier, consisting of multiple SVM classifiers trained for different face spoof attacks (e.g., printed photo and replayed video), is used to distinguish between genuine (live) and spoof faces. The proposed approach is extended to multiframe face spoof detection in videos using a voting-based scheme. We also collect a face spoof database, MSU mobile face spoofing database (MSU MFSD), using two mobile devices (Google Nexus 5 and MacBook Air) with three types of spoof attacks (printed photo, replayed video with iPhone 5S, and replayed video with iPad Air). Experimental results on two public-domain face spoof databases (Idiap REPLAY-ATTACK and CASIA FASD), and the MSU MFSD database show that the proposed approach outperforms the state-of-the-art methods in spoof detection. Our results also highlight the difficulty in separating genuine and spoof faces, especially in cross-database and cross-device scenarios.},
  keywords = {type:2D anti-spoofing experiment,Face recognition, spoof detection, image distortion analysis, ensemble classifier, cross-database, cross-device}
}


@article{galbally2016three,
  title={Three-dimensional and two-and-a-half-dimensional face recognition spoofing using three-dimensional printed models},
  author={Galbally, Javier and Satta, Riccardo},
  journal={IET Biometrics},
  volume={5},
  number={2},
  pages={83--91},
  year={2016},
  publisher={Wiley Online Library},
  doi = {10.1049/iet-bmt.2014.0075},
  url = {https://doi.org/10.1049/iet-bmt.2014.0075},
  abstract = {The vulnerability of biometric systems to external attacks using a physical artefact in order to impersonate the legitimate user has become a major concern over the last decade. Such a threat, commonly known as ‘spoofing’, poses a serious risk to the integrity of biometric systems. The usual low-complexity and low-cost characteristics of these attacks make them accessible to the general public, rendering each user a potential intruder. The present study addresses the spoofing issue analysing the feasibility to perform low-cost attacks with self-manufactured three-dimensional (3D) printed models to 2.5D and 3D face recognition systems. A new database with 2D, 2.5D and 3D real and fake data from 26 subjects was acquired for the experiments. Results showed the high vulnerability of the three tested systems, including a commercial solution, to the attacks.},
  keywords = {type:3D or 3D combine 2D spoofing experiment, biometrics (access control), security of data, face recognition, data acquisition, biometric systems, low-cost characteristics, self-manufactured three-dimensional printed model, external attacks, fake data acquisition, low-complexity characteristics, 3D face recognition systems, 2.5D face recognition systems, two-and-a-half-dimensional face recognition spoofing, legitimate user, physical artefact}
}



@INPROCEEDINGS{Atoum2017,
  author={Atoum, Yousef and Liu, Yaojie and Jourabloo, Amin and Liu, Xiaoming},
  booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},
  title={Face anti-spoofing using patch and depth-based CNNs},
  year={2017},
  pages={319-328},
  keywords={type:3D or 3D combine 2D anti-spoofing experiment, Face, Feature extraction, Videos, Face recognition, Image color analysis, Estimation, Training},
  doi={10.1109/BTAS.2017.8272713},
  url={https://doi.org/10.1109/BTAS.2017.8272713},
  abstract={The face image is the most accessible biometric modality which is used for highly accurate face recognition systems, while it is vulnerable to many different types of presentation attacks. Face anti-spoofing is a very critical step before feeding the face image to biometric systems. In this paper, we propose a novel two-stream CNN-based approach for face anti-spoofing, by extracting the local features and holistic depth maps from the face images. The local features facilitate CNN to discriminate the spoof patches independent of the spatial face areas. On the other hand, holistic depth map examine whether the input image has a face-like depth. Extensive experiments are conducted on the challenging databases (CASIA-FASD, MSU-USSA, and Replay Attack), with comparison to the state of the art.},
  isbn = {978-1-5386-1124-1},
  issn = {2474-9699}
}



@INPROCEEDINGS{Parkin2019,
  author={Parkin, Aleksandr and Grinchuk, Oleg},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title={Recognizing Multi-Modal Face Spoofing With Face Recognition Networks},
  year={2019},
  pages={1617--1623},
  keywords={type:3D or 3D combine 2D anti-spoofing experiment, Face, Face recognition, Task analysis, Training, Cameras, Training data, Neural networks},
  doi={10.1109/CVPRW.2019.00204},
  publisher={IEEE},
  location={Long Beach, CA, USA},
  conference_date={16-17 June 2019},
  ISBN={978-1-7281-2506-0, 978-1-7281-2507-7},
  ISSN={2160-7516, 2160-7508},
  url={https://doi.org/10.1109/CVPRW.2019.00204},
  abstract={Detecting spoofing attacks plays a vital role for deploying automatic face recognition for biometric authentication in applications such as access control, face payment, device unlock, etc. In this paper we propose a new anti-spoofing network architecture that takes advantage of multi-modal image data and aggregates intra-channel features at multiple network layers. We also transfer strong facial features learned for face recognition and show their benefits for detecting spoofing attacks. Finally, to increase the generalization ability of our method to unseen attacks, we use an ensemble of models trained separately for distinct types of spoofing attacks. The proposed method achieves state-of-the-art result on the largest multi-modal anti-spoofing dataset CASIA-SURF.}
}


@article{he2023lightweight,
  title={Lightweight network-based multi-modal feature fusion for face anti-spoofing},
  author={He, Dan and He, Xiping and Yuan, Rui and Li, Yue and Shen, Chao},
  journal={The Visual Computer},
  volume={39},
  number={4},
  pages={1423--1435},
  year={2023},
  publisher={Springer},
  doi={10.1007/s00371-022-02420-6},
  url={https://doi.org/10.1007/s00371-022-02420-6},
  keywords={type:3D or 3D combine 2D anti-spoofing experiment,Face anti-spoofing, Lightweight network, Multi-modal, Patch-level images, Redundant feature},
  abstract={Effectively identifying attacked faces is an urgent problem to be solved in the scenario of face recognition. As deep learning is applied to face anti-spoofing (FAS), many multi-modal approaches have been proven to be more efficient than single-modal, but at the same time, multi-modal approaches require a huge number of parameters and therefore result in high computation. To tackle this problem, a lightweight network-based multi-modal FAS model was proposed, which took patch-level images from multi-modal images (YCbCr, Depth, and IR) as the input to different branches, and designed a lightweight feature extraction module to solve the redundancy of feature maps extracted by the filters. Finally, an attention-based feature fusion module was constructed to fuse and classify the features extracted by each branch network. A great number of comparative experimental results demonstrated that this method greatly reduced parameters at a high accuracy. For example, the accuracy on single-modal datasets (Replay-Attack and CASIA-FASD) is 100%, and that on multi-modal dataset (CASIA-SURF) is 98.1269% (TPR@FPR = 10e-4) and 0.2232%. In addition, the backbone network has only 0.25 M parameters and 0.37 G FLOPs.}
}
